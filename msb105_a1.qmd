---
title: "Is reproducibility good enough?"
author: "Karin Liang"
date: last-modified
date-format: "dddd D MMM, YYYY"
papersize: a4
csl: apa7.csl
lang: en-GB
citeproc: true
format:
  html: 
    mainfont: "Helvetica"
  typst:
    mainfont: "Cambria"
  pdf:
    documentclass: article
    mainfont: "Hoefler Text"
    number-sections: true
    include-in-header:
      text: |
        \setcounter{section}{1}
    keep-tex: true
    fig-pos: "H"
    pdf-engine: xelatex
  docx:
    default: true
bibliography: 
  - my_references.bib
  - reproducibility_1.bib
  - reproducibility_2.bib
abstract: "This paper asks, “Is reproducibility good enough?” It reviews key literature, discusses whether reproducibility should be treated as the norm, and considers tools such as Quarto. The conclusion is that reproducibility is essential but not sufficient; replicability and broader reforms are also required for credible science."
---

## Introduction

This paper is based on the lecture "Reproducibility" in the MSB105 Data Science at the Western Norway University of Applied Sciences, which addresses the central question: "Is Reproducibility Good Enough?" We review the literature on reproducibility and replicability, and notes that many studies have reported difficulties in repeating or independently verifying published results. Such evidence suggests that reproducibility alone is insufficient for ensuring reliable knowledge. The paper also considers whether replicability should be treated as a scientific norm, examines the role of tools such as Quarto in supporting reproducible workflows, and briefly points to technical and cultural challenges. Finally, it outlines potential solutions, including dynamic documents, data citation, and institutional reforms to strengthen scientific credibility.

Reliable knowledge is the cornerstone of scientific progress. When findings cannot be verified, the path of research becomes uncertain and future decisions risk being misguided. Both reproducibility and replicability are needed to keep science trustworthy.

## Literature review

Reproducibility has been described as a core requirement of trustworthy science [@mcnutt2014]. Still, as @peng2011 emphasizes, it is a necessary but not sufficient condition for replicability. The National Science Foundation distinguishes reproducibility, replicability, and generalizability as complementary dimensions of robust science [@bollen2015; @nsf16137], and @goodman2016 emphasizes methods reproducibility, results reproducibility, and robustness as distinct but interrelated concepts.

Serious obstacles remain. Publication bias leads to the overrepresentation of studies with Type I errors [@rosenthal1979]. The consequences of Type I errors are severe, including persistence in the literature, wasted resources, and damage to scientific credibility [@simmons2011]. Meta-analysis, defined as a statistical method for synthesizing results across studies [@wikipedia2020], is also distorted when publication bias is present [@iyengar1988]. In addition, Ioannidis argued that most published research findings may actually be false [@ioannidis2005], and in medicine, irreproducible preclinical studies have been shown to waste resources and delay progress [@begley2012raise].

Large replication efforts confirm these concerns. In psychology, replication rates are much lower than in the original studies [@nosek2015; @klein2018], while in economics, attempts often fail due to missing data, poor documentation, or flawed code [@mccullough2008], and even when data and code are provided, successful replication is rare. Surveys across disciplines further show that most researchers have failed to reproduce another scientist’s experiments [@baker2016].

Technical solutions have been proposed. Research objects and data citation systems integrate and track research components [@bechhofer2013; @brase2009]. Literate programming [@knuth1992] and its derivatives such as noweb [@ramsey], Sweave [@leisch2002], knitr [@xie2014; @xie2015; @xie2020], and R Markdown [@xie2018; @allaire2020] paved the way for Quarto, which supports multi-language workflows and reproducible documents. Yet studies show that even with code and data available, replication can fail [@pimentel2019large; @rule2018exploration].

Cultural reforms are equally crucial. Guidelines for authors and editors have been proposed to reduce false positives [@simmons2011], while broader initiatives call for preregistration, data sharing, and openness [@munafo2017manifesto]. As noted by @hardwicke2020calibrating, lasting improvements in efficiency and credibility will depend not just on technical solutions but also on broader institutional and cultural change.

## Discussion of the reseach question

### Should replicability be the norm or is this to much to ask for now?

Replicability should be regarded as a scientific norm rather than an optional goal. While reproducibility is widely recognized as a baseline [@peng2011], it cannot ensure reliable findings on its own. Evidence from psychology, medicine, and economics shows that many published results fail independent verification [@nosek2015; @ioannidis2005; @mccullough2008], and replication success remains limited even when code and data are provided [@pimentel2019large].

The implications are serious: false positives and selective reporting waste resources and undermine confidence in research [@simmons2011]. As noted by @nsf16137, robust science requires more than isolated reproducibility, and @bollen2015 stresses that credibility depends on reproducibility, replicability, and generalizability together.

Possible remedies already exist. Proposals include author and editor guidelines to reduce false positives [@simmons2011], research objects and data citation systems [@bechhofer2013; @brase2009], literate programming traditions [@knuth1992], and broader reforms promoted by meta-research and open science initiatives [@hardwicke2020calibrating; @munafo2017manifesto]. These efforts suggest that normalizing replicability is not excessive but a necessary step toward credible science.

### Can Quarto documents help with reproducibility?

Quarto offers a powerful framework to improve reproducibility by integrating code, data, and analysis into one document, reducing problems caused by fragmented research outputs [@bechhofer2013]. It continues the literate programming tradition [@knuth1992], evolving from earlier tools like knitr [@xie2014; @xie2015] and R Markdown [@xie2018; @xie2020], which first enabled the seamless combination of code and narrative.

Still, reproducibility cannot be guaranteed simply by sharing code and data. Studies of computational notebooks reveal that many analyses fail to rerun due to missing dependencies or poor documentation [@pimentel2019large]. Similar shortcomings have been observed in economics, where archives rarely supported full replication [@mccullough2008]. Thus, while Quarto reduces technical barriers, it cannot by itself resolve issues such as publication bias [@rosenthal1979] or false positives [@simmons2011].

Where Quarto shows its greatest strength is in aligning with open science practices. It supports version control, citation management, and multiple output formats [@xie2018; @allaire2020], enabling consistent communication of results. More importantly, it complements wider reforms promoted by meta-research [@hardwicke2020calibrating] and open science initiatives advocating transparency, preregistration, and data sharing [@munafo2017manifesto]. In this way, Quarto functions not as a complete solution, but as a key step toward more trustworthy science.

### What problems remains and how can these be solved?

Even with progress in reproducibility, significant challenges remain. Structural incentives still prioritize novel, positive results, encouraging publication bias and sustaining false positives, which undermines efficiency and damages trust in science [@rosenthal1979; @simmons2011]. Technical hurdles also persist: differences in software versions, dependencies, and computing environments often prevent replication even when data and code are shared [@pimentel2019large; @mccullough2008; @nosek2015]. Moreover, replication work often lacks cultural and institutional support, while limited transparency, preregistration, and open data sharing further discourage such efforts [@hardwicke2020calibrating; @munafo2017manifesto; @baker2016].

Solutions require both technical and cultural strategies. On the technical side, tools like Quarto, along with predecessors such as R Markdown and knitr [@xie2014; @xie2015; @xie2018; @xie2020; @allaire2020], help integrate code, results, and text, supporting reproducibility across platforms. Culturally, change must come through author and editor guidelines to limit false positives [@simmons2011; @munafo2017manifesto], systemic reforms promoting openness [@hardwicke2020calibrating], and adoption of practices such as data citation [@brase2009]. Meta-research has outlined concrete steps in this direction, including stronger peer review, preregistration, and broader sharing of code and data [@hardwicke2020calibrating].

## Conclusion

This paper reviewed the distinction between reproducibility and replicability, the contribution of Quarto, and the remaining barriers to reliable science. The evidence shows that reproducibility is necessary but insufficient, as many published research results fail to be independently verified [@ioannidis2005; @nosek2015; @mccullough2008]. Replicability and generalizability must also be part of the standard for credible research [@bollen2015].

Quarto and earlier tools such as knitr and R Markdown provide valuable technical support by combining code, data, and text into dynamic documents [@xie2014; @xie2018; @allaire2020]. These tools reduce fragmentation but cannot alone solve deeper problems like publication bias and false positives [@rosenthal1979; @simmons2011].

Lasting progress depends on both technical and cultural reforms.Dynamic documents, environment management, and data citation systems improve reproducibility, while preregistration, open sharing, and systemic incentives strengthen replicability [@munafo2017manifesto; @hardwicke2020calibrating].

In sum, reproducibility is not enough. Science must normalize replicability and embed openness into its practices to ensure long-term credibility.

### Software and Packages

- R version 4.5.1 (2025-06-13)  
- Base packages: stats, graphics, grDevices, utils, datasets, methods, base  

## References

