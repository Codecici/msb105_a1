---
title: "Is reproducibility good enough?"
author: "Karin Liang"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf:
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography:
  - reproducibility_1.bib
  - reproducibility_2.bib
  - my_references.bib
abstract: "A very short abstract. Put the abstract text here. One or two paragraphs summarising what follows below."
---

## Introduction

What is this paper about?

This paper is based on the lecture "Reproducibility" from the MSB105 Data Science at the Western Norway University of Applied Sciences, and explores the question, "Is Reproducibility Good Enough?"

What is discussed?




Why is it of any consequence?




## Literature review

Smart stuff from others about the topic.

Use a least 20 citations, a least 5 of them must be new (not from the provided .bib file).

Use both in-line and normal citations.

Example:

@gentleman2005 argues that bla bla bla.
On the other hand it's claimed that bla bla [@barbalorenaa.2018; @bartlett2008].


1. Science advances on a foundation of trusted discoveries, but some reports indicate that a significant portion of research findings cannot be reproduced. [@mcnutt2014]

2. @peng2011 argues reproducibility is a necessary, but not sufficient, condition for replicability. Without reproducibility, there can be no replicability. Reproducibility has already become a critical issue in the scientific community.

3. @bollen2015 emphasizes that robust and reliable scientific research must satisfy three fundamental criteria.
First, reproducibility requires that researchers can obtain consistent results when using the same original data and analytical procedures. 
Second, replicability demands that independent researchers following the same methodological protocols but collecting new data should arrive at substantively consistent findings.
Finally, generalizability examines whether research conclusions remain valid when applied to different contexts, populations, or conditions beyond the original study setting.


4. @nsf16137 emphasizes that robust and reliable research serves as the cornerstone for all scientific development and progress.


5. @goodman2016 points out that research reproducibility can be explained through two key dimensions:
-Methods reproducibility requires transparent research procedures and data to enable exact replication of the study.
-Results reproducibility emphasizes obtaining consistent conclusions through independent replication studies.

Additionally:
-Robustness refers to the stability of research conclusions when core assumptions or experimental conditions change.
-Generalizability indicates whether experimental effects persist beyond the original study context or population.


6.Publication bias leads to the excessive publication of studies with Type I errors (false positives). Specifically, studies supporting the "no effect" hypothesis (H₀) — i.e., negative results — tend to be shelved due to the "file drawer problem," [@rosenthal1979], while only results showing statistical significance (which may be false positives) are likely to be published.

7. The severe consequences of Type I errors (false positives) in scientific practice mainly include three aspects: persistence, resource waste, and credibility damage.[@simmons2011]

8. @wikipedia2020 points out that a meta-analysis as statistical analysis that combines the results of multiple scientific studies. But @iyengar1988 argues that if there is a publication bias this will also influence meta-analysis.

9. @simmons2011 proposed a solution to the "replication crisis," which includes six requirements for authors and four guidelines for editors.

10. @ioannidis2005 argues that in contemporary scientific practice, the majority—or even the vast majority—of published research claims may be false.

11. @nosek2015 employed high-powered experimental designs and obtained materials provided by the original authors, yet the majority of replication attempts failed to reproduce evidence supporting the original findings.

12.@klein2018 found that 15 (54%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding.

13.[@mccullough2008] All the long-standing archives at economics journals do not facilitate the re-production of published results. Even when both data and code are provided (e.g., the Federal Reserve Bank of St. Louis Review), successful replication remains rare.

14.[@bechhofer2013] Research Objects bundle all data, code, software environments, workflows, and even the research paper itself into a complete "box," allowing anyone to easily reproduce the entire work — effectively solving the problem of content fragmentation.

15.[@brase2009] Assigning a unique Digital Object Identifier (DOI) to datasets enables their formal and standardized citation, just like research papers — effectively addressing the issue of "data citation and incentives".

16. @knuth1992 pioneered the concept of literate programming, advocating for the deep integration of code and documentation to ensure computational transparency and reproducibility from the source. [@ramsey] The noweb tool later simplified Knuth’s literate programming approach, making it more lightweight, language-agnostic, and accessible—effectively lowering the technical barriers to creating executable research documents.


17. @begley2012raise reveals severe reproducibility issues in the field.

18. @hardwicke2020calibrating argues that improving the efficiency and credibility of science requires not only technical tools but also institutional and cultural reforms.

19.@baker2016 reveals that more than 70% of researchers failed to replicate another scientist’s experiments.

20.@munafo2017manifesto proposes a “manifesto” for reproducible science, highlighting the need for transparency, preregistration, and open data sharing. 

21.@pimentel2019large analyzes over 1.4 million Jupyter notebooks on GitHub, the study found that only about 24% ran without errors and just 4% produced the same results.

22.@rule2018exploration shows that computational notebooks such as Jupyter supports flexible analysis, this practice limits reproducibility and the effective communication of results.


## Discussion of the reseach question

-   Should replicability be the norm or is this to much to ask for now?
-   Can Quarto documents help with reproducibility?
-   What problems remains and how can these be solved?




## Conclusion





## References

and

-   Version number and reference to packages used
-   R version used
