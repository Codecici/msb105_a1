---
title: "Is reproducibility good enough?"
author: "Karin Liang"
date: last-modified
date-format: "dddd D MMM, YYYY"
csl: apa7.csl
lang: en-GB
format:
  html: default
  typst:
    papersize: a4
  pdf:
    documentclass: article
    number-sections: true
    keep-tex: true
    papersize: A4
    fig-pos: "H"
bibliography:
  - reproducibility_1.bib
  - reproducibility_2.bib
  - my_references.bib
abstract: "A very short abstract. Put the abstract text here. One or two paragraphs summarising what follows below."
---

## Introduction

What is this paper about?

This paper is based on the lecture "Reproducibility" in the MSB105 Data Science at the Western Norway University of Applied Sciences, addresses the central question: "Is Reproducibility Good Enough?"

What is discussed?

This paper reviews the literature on reproducibility and replicability, and notes that many studies have reported difficulties in repeating or independently verifying published results. Such evidence suggests that reproducibility alone is insufficient for ensuring reliable knowledge. The paper also considers whether replicability should be treated as a scientific norm, examines the role of tools such as Quarto in supporting reproducible workflows, and briefly points to technical and cultural challenges. Finally, it outlines potential solutions, including dynamic documents, data citation, and institutional reforms to strengthen scientific credibility.


Why is it of any consequence?

Reliable knowledge is the cornerstone of scientific progress. When findings cannot be verified, the path of research becomes uncertain and future decisions risk being misguided. Both reproducibility and replicability are needed to keep science trustworthy.


## Literature review

Reproducibility has been described as a core requirement of trustworthy science [@mcnutt2014]. Still, as @peng2011 notes, it is only a minimal foundation for replicability. The National Science Foundation distinguishes reproducibility, replicability, and generalizability as complementary dimensions of robust science [@bollen2015; @nsf16137], and @goodman2016 emphasizes methods reproducibility, results reproducibility, and robustness as distinct but interrelated concepts.

Serious obstacles remain. Publication bias results in overrepresentation of positive findings and neglect of null results [@rosenthal1979], inflating false positives and undermining meta-analyses [@iyengar1988]. Undisclosed analytic flexibility further contributes to unreliable results [@simmons2011]. Ioannidis argued that most published findings may in fact be false [@ioannidis2005], while in medicine, @begley2012raise documented how irreproducible preclinical studies waste resources.

Large replication efforts confirm these concerns. In psychology, replication rates are far lower than originally reported [@nosek2015; @klein2018], while economics studies reveal failures tied to missing data and poor documentation [@mccullough2008]. Surveys of researchers across disciplines reinforce that reproducibility is a persistent problem [@baker2016].

Technical solutions have been proposed. Research objects and data citation systems integrate and track research components [@bechhofer2013; @brase2009]. Literate programming [@knuth1992] and its derivatives such as noweb [@ramsey], Sweave [@leisch2002], knitr [@xie2014; @xie2015; @xie2020], and R Markdown [@xie2018; @allaire2020] paved the way for Quarto, which supports multi-language workflows and reproducible documents. Yet studies show that even with code and data available, replication may fail due to dependency management or poor documentation [@pimentel2019large; @rule2018exploration].

Cultural reforms are equally crucial. Guidelines for authors and editors have been proposed to reduce false positives [@simmons2011], while broader initiatives call for preregistration, data sharing, and openness [@munafo2017manifesto]. Meta-research emphasizes systemic change to align incentives with transparency and reliability [@hardwicke2020calibrating].





## Discussion of the reseach question

-   Should replicability be the norm or is this to much to ask for now?

Replicability should be regarded as a scientific norm rather than an optional goal. While reproducibility is widely recognized as a baseline [@peng2011], it cannot ensure reliable findings on its own. Evidence from psychology, medicine, and economics shows that many published results fail independent verification [@nosek2015; @ioannidis2005; @mccullough2008], and replication success remains limited even when code and data are provided [@pimentel2019large].

The implications are serious: false positives and selective reporting waste resources and undermine confidence in research [@simmons2011]. As noted by @nsf16137, robust science requires more than isolated reproducibility, and @bollen2015 stresses that credibility depends on reproducibility, replicability, and generalizability together.

Possible remedies already exist. Proposals include author and editor guidelines to reduce false positives [@simmons2011], research objects and data citation systems [@bechhofer2013; @brase2009], literate programming traditions [@knuth1992], and broader reforms promoted by meta-research and open science initiatives [@hardwicke2020calibrating; @munafo2017manifesto]. These efforts suggest that normalizing replicability is not excessive but a necessary step toward credible science.


-   Can Quarto documents help with reproducibility?

Quarto offers a powerful framework to improve reproducibility by integrating code, data, and analysis into one document, reducing problems caused by fragmented research outputs [@bechhofer2013]. It continues the literate programming tradition [@knuth1992], evolving from earlier tools like knitr [@xie2014; @xie2015] and R Markdown [@xie2018; @xie2020], which first enabled the seamless combination of code and narrative.

Still, reproducibility cannot be guaranteed simply by sharing code and data. Studies of computational notebooks reveal that many analyses fail to rerun due to missing dependencies or poor documentation [@pimentel2019large]. Similar shortcomings have been observed in economics, where archives rarely supported full replication [@mccullough2008]. Thus, while Quarto reduces technical barriers, it cannot by itself resolve issues such as publication bias [@rosenthal1979] or false positives [@simmons2011].

Where Quarto shows its greatest strength is in aligning with open science practices. It supports version control, citation management, and multiple output formats [@xie2018; @allaire2020], enabling consistent communication of results. More importantly, it complements wider reforms promoted by meta-research [@hardwicke2020calibrating] and open science initiatives advocating transparency, preregistration, and data sharing [@munafo2017manifesto]. In this way, Quarto functions not as a complete solution, but as a key step toward more trustworthy science.


-   What problems remains and how can these be solved?

Even with progress in reproducibility, significant challenges remain. Structural incentives still prioritize novel, positive results, encouraging publication bias and sustaining false positives, which undermines efficiency and damages trust in science [@rosenthal1979; @simmons2011]. Technical hurdles also persist: differences in software versions, dependencies, and computing environments often prevent replication even when data and code are shared [@pimentel2019large; @mccullough2008; @nosek2015]. Moreover, replication work often lacks cultural and institutional support, while limited transparency, preregistration, and open data sharing further discourage such efforts [@hardwicke2020calibrating; @munafo2017manifesto; @baker2016].

Solutions require both technical and cultural strategies. On the technical side, tools like Quarto, along with predecessors such as R Markdown and knitr [@xie2014; @xie2015; @xie2018; @xie2020; @allaire2020], help integrate code, results, and text, supporting reproducibility across platforms. Culturally, change must come through author and editor guidelines to limit false positives [@simmons2011; @munafo2017manifesto], systemic reforms promoting openness [@hardwicke2020calibrating], and adoption of practices such as data citation [@brase2009]. Meta-research has outlined concrete steps in this direction, including stronger peer review, preregistration, and broader sharing of code and data [@hardwicke2020calibrating].

## Conclusion

This paper reviewed the distinction between reproducibility and replicability, the contribution of Quarto, and the remaining barriers to reliable science. The evidence shows that reproducibility is necessary but insufficient, as many published research results fail to be independently verified [@ioannidis2005; @nosek2015; @mccullough2008]. Replicability and generalizability must also be part of the standard for credible research [@bollen2015].

Quarto and earlier tools such as knitr and R Markdown provide valuable technical support by combining code, data, and text into dynamic documents [@xie2014; @xie2018; @allaire2020]. These tools reduce fragmentation but cannot alone solve deeper problems like publication bias and false positives [@rosenthal1979; @simmons2011].

Lasting progress depends on both technical and cultural reforms.Dynamic documents, environment management, and data citation systems improve reproducibility, while preregistration, open sharing, and systemic incentives strengthen replicability [@munafo2017manifesto; @hardwicke2020calibrating].

In sum, reproducibility is not enough. Science must normalize replicability and embed openness into its practices to ensure long-term credibility.



## References

and

-   Version number and reference to packages used
-   R version used
