% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Hoefler Text}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\setcounter{section}{1}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{british}
\ifPDFTeX
\else
\babelfont{rm}[]{Hoefler Text}
\fi
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\ifLuaTeX
  \usepackage[english]{selnolig} % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Is reproducibility good enough?},
  pdfauthor={Karin Liang},
  pdflang={en-GB},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Is reproducibility good enough?}
\author{Karin Liang}
\date{Tuesday 16 Sep, 2025}

\begin{document}
\maketitle
\begin{abstract}
This paper asks, ``Is reproducibility good enough?'' It reviews key
literature, discusses whether reproducibility should be treated as the
norm, and considers tools such as Quarto. The conclusion is that
reproducibility is essential but not sufficient; replicability and
broader reforms are also required for credible science.
\end{abstract}


\section{Introduction}\label{introduction}

This paper is based on the lecture ``Reproducibility'' in the MSB105
Data Science at the Western Norway University of Applied Sciences, which
addresses the central question: ``Is Reproducibility Good Enough?'' We
review the literature on reproducibility and replicability, and notes
that many studies have reported difficulties in repeating or
independently verifying published results. Such evidence suggests that
reproducibility alone is insufficient for ensuring reliable knowledge.
The paper also considers whether replicability should be treated as a
scientific norm, examines the role of tools such as Quarto in supporting
reproducible workflows, and briefly points to technical and cultural
challenges. Finally, it outlines potential solutions, including dynamic
documents, data citation, and institutional reforms to strengthen
scientific credibility.

Reliable knowledge is the cornerstone of scientific progress. When
findings cannot be verified, the path of research becomes uncertain and
future decisions risk being misguided. Both reproducibility and
replicability are needed to keep science trustworthy.

\section{Literature review}\label{literature-review}

Reproducibility has been described as a core requirement of trustworthy
science (McNutt, 2014). Still, as Peng (2011) emphasizes, it is a
necessary but not sufficient condition for replicability. The National
Science Foundation distinguishes reproducibility, replicability, and
generalizability as complementary dimensions of robust science (Bollen
et al., 2015; \emph{Dear {Colleague Letter}}, n.d.), and Goodman et al.
(2016) emphasizes methods reproducibility, results reproducibility, and
robustness as distinct but interrelated concepts.

Serious obstacles remain. Publication bias leads to the
overrepresentation of studies with Type I errors (Rosenthal, 1979). The
consequences of Type I errors are severe, including persistence in the
literature, wasted resources, and damage to scientific credibility
(Simmons et al., 2011). Meta-analysis, defined as a statistical method
for synthesizing results across studies (wikipedia, 2020), is also
distorted when publication bias is present (Iyengar \& Greenhouse,
1988). In addition, Ioannidis argued that most published research
findings may actually be false (Ioannidis, 2005), and in medicine,
irreproducible preclinical studies have been shown to waste resources
and delay progress (Begley \& Ellis, 2012).

Large replication efforts confirm these concerns. In psychology,
replication rates are much lower than in the original studies (Klein et
al., 2018; Nosek et al., 2015), while in economics, attempts often fail
due to missing data, poor documentation, or flawed code (McCullough et
al., 2008), and even when data and code are provided, successful
replication is rare. Surveys across disciplines further show that most
researchers have failed to reproduce another scientist's experiments
(Baker, 2016).

Technical solutions have been proposed. Research objects and data
citation systems integrate and track research components (Bechhofer et
al., 2013; Brase, 2009). Literate programming (Knuth, 1992) and its
derivatives such as noweb (Ramsey, n.d.), Sweave (Leisch, 2002), knitr
(Xie, 2014, 2015, 2020), and R Markdown (Allaire et al., 2020; Xie et
al., 2018) paved the way for Quarto, which supports multi-language
workflows and reproducible documents. Yet studies show that even with
code and data available, replication can fail (Pimentel et al., 2019;
Rule et al., 2018).

Cultural reforms are equally crucial. Guidelines for authors and editors
have been proposed to reduce false positives (Simmons et al., 2011),
while broader initiatives call for preregistration, data sharing, and
openness (Munafò et al., 2017). As noted by Hardwicke et al. (2020),
lasting improvements in efficiency and credibility will depend not just
on technical solutions but also on broader institutional and cultural
change.

\section{Discussion of the reseach
question}\label{discussion-of-the-reseach-question}

\subsection{Should replicability be the norm or is this to much to ask
for
now?}\label{should-replicability-be-the-norm-or-is-this-to-much-to-ask-for-now}

Replicability should be regarded as a scientific norm rather than an
optional goal. While reproducibility is widely recognized as a baseline
(Peng, 2011), it cannot ensure reliable findings on its own. Evidence
from psychology, medicine, and economics shows that many published
results fail independent verification (Ioannidis, 2005; McCullough et
al., 2008; Nosek et al., 2015), and replication success remains limited
even when code and data are provided (Pimentel et al., 2019).

The implications are serious: false positives and selective reporting
waste resources and undermine confidence in research (Simmons et al.,
2011). As noted by \emph{Dear {Colleague Letter}} (n.d.), robust science
requires more than isolated reproducibility, and Bollen et al. (2015)
stresses that credibility depends on reproducibility, replicability, and
generalizability together.

Possible remedies already exist. Proposals include author and editor
guidelines to reduce false positives (Simmons et al., 2011), research
objects and data citation systems (Bechhofer et al., 2013; Brase, 2009),
literate programming traditions (Knuth, 1992), and broader reforms
promoted by meta-research and open science initiatives (Hardwicke et
al., 2020; Munafò et al., 2017). These efforts suggest that normalizing
replicability is not excessive but a necessary step toward credible
science.

\subsection{Can Quarto documents help with
reproducibility?}\label{can-quarto-documents-help-with-reproducibility}

Quarto offers a powerful framework to improve reproducibility by
integrating code, data, and analysis into one document, reducing
problems caused by fragmented research outputs (Bechhofer et al., 2013).
It continues the literate programming tradition (Knuth, 1992), evolving
from earlier tools like knitr (Xie, 2014, 2015) and R Markdown (Xie et
al., 2018; Xie, 2020), which first enabled the seamless combination of
code and narrative.

Still, reproducibility cannot be guaranteed simply by sharing code and
data. Studies of computational notebooks reveal that many analyses fail
to rerun due to missing dependencies or poor documentation (Pimentel et
al., 2019). Similar shortcomings have been observed in economics, where
archives rarely supported full replication (McCullough et al., 2008).
Thus, while Quarto reduces technical barriers, it cannot by itself
resolve issues such as publication bias (Rosenthal, 1979) or false
positives (Simmons et al., 2011).

Where Quarto shows its greatest strength is in aligning with open
science practices. It supports version control, citation management, and
multiple output formats (Allaire et al., 2020; Xie et al., 2018),
enabling consistent communication of results. More importantly, it
complements wider reforms promoted by meta-research (Hardwicke et al.,
2020) and open science initiatives advocating transparency,
preregistration, and data sharing (Munafò et al., 2017). In this way,
Quarto functions not as a complete solution, but as a key step toward
more trustworthy science.

\subsection{What problems remains and how can these be
solved?}\label{what-problems-remains-and-how-can-these-be-solved}

Even with progress in reproducibility, significant challenges remain.
Structural incentives still prioritize novel, positive results,
encouraging publication bias and sustaining false positives, which
undermines efficiency and damages trust in science (Rosenthal, 1979;
Simmons et al., 2011). Technical hurdles also persist: differences in
software versions, dependencies, and computing environments often
prevent replication even when data and code are shared (McCullough et
al., 2008; Nosek et al., 2015; Pimentel et al., 2019). Moreover,
replication work often lacks cultural and institutional support, while
limited transparency, preregistration, and open data sharing further
discourage such efforts (Baker, 2016; Hardwicke et al., 2020; Munafò et
al., 2017).

Solutions require both technical and cultural strategies. On the
technical side, tools like Quarto, along with predecessors such as R
Markdown and knitr (Allaire et al., 2020; Xie, 2014, 2015, 2020; Xie et
al., 2018), help integrate code, results, and text, supporting
reproducibility across platforms. Culturally, change must come through
author and editor guidelines to limit false positives (Munafò et al.,
2017; Simmons et al., 2011), systemic reforms promoting openness
(Hardwicke et al., 2020), and adoption of practices such as data
citation (Brase, 2009). Meta-research has outlined concrete steps in
this direction, including stronger peer review, preregistration, and
broader sharing of code and data (Hardwicke et al., 2020).

\section{Conclusion}\label{conclusion}

This paper reviewed the distinction between reproducibility and
replicability, the contribution of Quarto, and the remaining barriers to
reliable science. The evidence shows that reproducibility is necessary
but insufficient, as many published research results fail to be
independently verified (Ioannidis, 2005; McCullough et al., 2008; Nosek
et al., 2015). Replicability and generalizability must also be part of
the standard for credible research (Bollen et al., 2015).

Quarto and earlier tools such as knitr and R Markdown provide valuable
technical support by combining code, data, and text into dynamic
documents (Allaire et al., 2020; Xie, 2014; Xie et al., 2018). These
tools reduce fragmentation but cannot alone solve deeper problems like
publication bias and false positives (Rosenthal, 1979; Simmons et al.,
2011).

Lasting progress depends on both technical and cultural reforms.Dynamic
documents, environment management, and data citation systems improve
reproducibility, while preregistration, open sharing, and systemic
incentives strengthen replicability (Hardwicke et al., 2020; Munafò et
al., 2017).

In sum, reproducibility is not enough. Science must normalize
replicability and embed openness into its practices to ensure long-term
credibility.

\subsection{Software and Packages}\label{software-and-packages}

\begin{itemize}
\tightlist
\item
  R version 4.5.1 (2025-06-13)\\
\item
  Base packages: stats, graphics, grDevices, utils, datasets, methods,
  base
\end{itemize}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-allaire2020}
Allaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins,
A., Wickham, H., Cheng, J., Chang, W., \& Iannone, R. (2020).
\emph{Rmarkdown: {Dynamic} documents for r}.

\bibitem[\citeproctext]{ref-baker2016}
Baker, M. (2016). 1,500 scientists lift the lid on reproducibility.
\emph{Nature}, \emph{533}, 452--454.
\url{https://doi.org/10.1038/533452a}

\bibitem[\citeproctext]{ref-bechhofer2013}
Bechhofer, S., Buchan, I., De Roure, D., Missier, P., Ainsworth, J.,
Bhagat, J., Couch, P., Cruickshank, D., Delderfield, M., Dunlop, I.,
Gamble, M., Michaelides, D., Owen, S., Newman, D., Sufi, S., \& Goble,
C. (2013). Why linked data is not enough for scientists. \emph{Future
Generation Computer Systems}, \emph{29}(2), 599--611.
\url{https://doi.org/10.1016/j.future.2011.08.004}

\bibitem[\citeproctext]{ref-begley2012raise}
Begley, C. G., \& Ellis, L. M. (2012). Raise standards for preclinical
cancer research. \emph{Nature}, \emph{483}(7391), 531--533.

\bibitem[\citeproctext]{ref-bollen2015}
Bollen, K., Cacioppo, J. T., Krosnick, J. A., Olds, J. L., \& Kaplan, R.
M. (2015). \emph{Social, {Behavioral}, and {Economic Sciences
Perspectives} on {Robust} and {Reliable Science}} (Report of the
Subcommittee on Replicability in Science Advisory Committee to the
National Science Foundation Directorate for Social, Behavioral, and
Economic Sciences). {NSF}.

\bibitem[\citeproctext]{ref-brase2009}
Brase, J. (2009). {DataCite} - {A Global Registration Agency} for
{Research Data}. \emph{2009 {Fourth International Conference} on
{Cooperation} and {Promotion} of {Information Resources} in {Science}
and {Technology}}, 257--261.
\url{https://doi.org/10.1109/COINFO.2009.66}

\bibitem[\citeproctext]{ref-nsf16137}
\emph{Dear {Colleague Letter}: {Robust} and {Reliable Research} in the
{Social}, {Behavioral}, and {Economic Sciences} (Nsf16137) \textbar{}
{NSF} - {National Science Foundation}}. (n.d.).
https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp.

\bibitem[\citeproctext]{ref-goodman2016}
Goodman, S. N., Fanelli, D., \& Ioannidis, J. P. A. (2016). What does
research reproducibility mean? \emph{Science Translational Medicine},
\emph{8}(341), 341ps12--341ps12.
\url{https://doi.org/10.1126/scitranslmed.aaf5027}

\bibitem[\citeproctext]{ref-hardwicke2020calibrating}
Hardwicke, T. E., Serghiou, S., Janiaud, P., Danchev, V., Crüwell, S.,
Goodman, S. N., \& Ioannidis, J. P. (2020). Calibrating the scientific
ecosystem through meta-research. \emph{Annual Review of Statistics and
Its Application}, \emph{7}(1), 11--37.

\bibitem[\citeproctext]{ref-ioannidis2005}
Ioannidis, J. P. A. (2005). Why {Most Published Research Findings Are
False}. \emph{PLOS Medicine}, \emph{2}(8), e124.
\url{https://doi.org/10.1371/journal.pmed.0020124}

\bibitem[\citeproctext]{ref-iyengar1988}
Iyengar, S., \& Greenhouse, J. B. (1988). Selection models and the file
drawer problem. \emph{Statistical Science}, \emph{3}(1), 109--117.

\bibitem[\citeproctext]{ref-klein2018}
Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Reginald B.
Adams, Jr., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahnı́k,
Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R.,
Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R.,
\ldots{} Nosek, B. A. (2018). Many labs 2: {Investigating} variation in
replicability across samples and settings. \emph{Advances in Methods and
Practices in Psychological Science}, \emph{1}(4), 443--490.
\url{https://doi.org/10.1177/2515245918810225}

\bibitem[\citeproctext]{ref-knuth1992}
Knuth, D. E. (1992). \emph{Literate {Programming}}. {Cambridge
University Press}.

\bibitem[\citeproctext]{ref-leisch2002}
Leisch, F. (2002). Sweave: {Dynamic Generation} of {Statistical Reports
Using Literate Data Analysis}. In W. Härdle \& B. Rönz (Eds.),
\emph{Compstat} (pp. 575--580). {Physica-Verlag HD}.
\url{https://doi.org/10.1007/978-3-642-57489-4_89}

\bibitem[\citeproctext]{ref-mccullough2008}
McCullough, B. D., McGeary, K. A., \& Harrison, T. D. (2008). Do
economics journal archives promote replicable research? \emph{Canadian
Journal of Economics/Revue Canadienne d'économique}, \emph{41}(4),
1406--1420. \url{https://doi.org/10.1111/j.1540-5982.2008.00509.x}

\bibitem[\citeproctext]{ref-mcnutt2014}
McNutt, M. (2014). Reproducibility. \emph{Science}, \emph{343}(6168),
229--229. \url{https://doi.org/10.1126/science.1250475}

\bibitem[\citeproctext]{ref-munafo2017manifesto}
Munafò, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C.
D., Percie du Sert, N., Simonsohn, U., Wagenmakers, E.-J., Ware, J. J.,
\& Ioannidis, J. P. (2017). A manifesto for reproducible science.
\emph{Nature Human Behaviour}, \emph{1}(1), 0021.

\bibitem[\citeproctext]{ref-nosek2015}
Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D.,
Breckler, S. J., Buck, S., Chambers, C. D., Chin, G., Christensen, G.,
Contestabile, M., Dafoe, A., Eich, E., Freese, J., Glennerster, R.,
Goroff, D., Green, D. P., Hesse, B., Humphreys, M., \ldots{} Yarkoni, T.
(2015). Promoting an open research culture. \emph{Science},
\emph{348}(6242), 1422--1425.
\url{https://doi.org/10.1126/science.aab2374}

\bibitem[\citeproctext]{ref-peng2011}
Peng, R. D. (2011). Reproducible {Research} in {Computational Science}.
\emph{Science}, \emph{334}(6060), 1226--1227.
\url{https://doi.org/10.1126/science.1213847}

\bibitem[\citeproctext]{ref-pimentel2019large}
Pimentel, J. F., Murta, L., Braganholo, V., \& Freire, J. (2019). A
large-scale study about quality and reproducibility of jupyter
notebooks. \emph{2019 IEEE/ACM 16th International Conference on Mining
Software Repositories (MSR)}, 507--517.

\bibitem[\citeproctext]{ref-ramsey}
Ramsey, N. (n.d.). \emph{Noweb home page}.
https://www.cs.tufts.edu/\textasciitilde nr/noweb/.

\bibitem[\citeproctext]{ref-rosenthal1979}
Rosenthal, R. (1979). \emph{The file drawer problem and tolerance for
null results.} \emph{86}, 638--641.

\bibitem[\citeproctext]{ref-rule2018exploration}
Rule, A., Tabard, A., \& Hollan, J. D. (2018). Exploration and
explanation in computational notebooks. \emph{Proceedings of the 2018
CHI Conference on Human Factors in Computing Systems}, 1--12.

\bibitem[\citeproctext]{ref-simmons2011}
Simmons, J. P., Nelson, L. D., \& Simonsohn, U. (2011). False-positive
psychology: {Undisclosed} flexibility in data collection and analysis
allows presenting anything as significant. \emph{Psychological Science},
\emph{22}(11), 1359--1366.
\url{https://doi.org/10.1177/0956797611417632}

\bibitem[\citeproctext]{ref-wikipedia2020}
wikipedia. (2020). Meta-analysis. \emph{Wikipedia}.

\bibitem[\citeproctext]{ref-xie2014}
Xie, Y. (2014). Knitr: {A} comprehensive tool for reproducible research
in {R}. In V. Stodden, F. Leisch, \& R. D. Peng (Eds.),
\emph{Implementing reproducible computational research}. {Chapman and
Hall/CRC}.

\bibitem[\citeproctext]{ref-xie2015}
Xie, Y. (2015). \emph{Dynamic documents with {R} and knitr} (Second).
{Chapman and Hall/CRC}.

\bibitem[\citeproctext]{ref-xie2020}
Xie, Y. (2020). \emph{Knitr: {A} general-purpose package for dynamic
report generation in r} {[}Manual{]}.

\bibitem[\citeproctext]{ref-xie2018}
Xie, Y., Allaire, J. J., \& Grolemund, G. (2018). \emph{R markdown:
{The} definitive guide}. {Chapman and Hall/CRC}.

\end{CSLReferences}




\end{document}
