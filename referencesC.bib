@article{markowetz2015,
	author = {Markowetz, Florian},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1186/s13059-015-0850-7},
	file = {/Users/ag/Zotero/storage/HPQDRJDG/Markowetz - 2015 - Five selfish reasons to work reproducibly.pdf},
	issn = {1474-760X},
	journal = {Genome Biology},
	language = {en},
	month = dec,
	number = {1},
	pages = {274},
	title = {Five Selfish Reasons to Work Reproducibly},
	volume = {16},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1186/s13059-015-0850-7}}

@article{mccullough2008,
	abstract = {Abstract. All the long-standing archives at economics journals do not facilitate the reproduction of published results. The data-only archives at Journal of Business and Economic Statistics and Economic Journal fail in part because most authors do not contribute data. Results published in the FRB St. Louis Review can rarely be reproduced using the data+code in the journal archive. Recently created archives at top journals should avoid the mistakes of their predecessors. We categorize reasons for archives' failures and identify successful policies.},
	annotation = {\_eprint: https://www.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5982.2008.00509.x},
	author = {McCullough, B. D. and McGeary, Kerry Anne and Harrison, Teresa D.},
	copyright = {\textcopyright{} Canadian Economics Association},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1111/j.1540-5982.2008.00509.x},
	file = {/Users/ag/Zotero/storage/YYVZQFP4/McCullough et al. - 2008 - Do economics journal archives promote replicable r.pdf;/Users/ag/Zotero/storage/KNNHELQT/j.1540-5982.2008.00509.html},
	issn = {1540-5982},
	journal = {Canadian Journal of Economics/Revue canadienne d'\'economique},
	keywords = {done},
	language = {en},
	number = {4},
	pages = {1406--1420},
	title = {Do Economics Journal Archives Promote Replicable Research?},
	volume = {41},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1111/j.1540-5982.2008.00509.x}}

@article{mcnutt2014,
	author = {McNutt, Marcia},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1126/science.1250475},
	eprint = {https://science.sciencemag.org/content/343/6168/229.full.pdf},
	issn = {0036-8075},
	journal = {Science},
	keywords = {done},
	number = {6168},
	pages = {229--229},
	publisher = {{American Association for the Advancement of Science}},
	title = {Reproducibility},
	volume = {343},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1126/science.1250475}}

@article{nosek2015,
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. L. and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1126/science.aab2374},
	file = {/Users/ag/Zotero/storage/FJJ7L9BR/Nosek et al. - 2015 - Promoting an open research culture.pdf},
	issn = {0036-8075, 1095-9203},
	journal = {Science},
	keywords = {done},
	language = {en},
	month = jun,
	number = {6242},
	pages = {1422--1425},
	title = {Promoting an Open Research Culture},
	volume = {348},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1126/science.aab2374}}

@article{nust2017,
	author = {N{\"u}st, Daniel and Konkol, Markus and Pebesma, Edzer and Kray, Christian and Schutzeichel, Marc and Przibytzin, Holger and Lorenz, J{\"o}rg},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1045/january2017-nuest},
	issn = {1082-9873},
	journal = {D-Lib Magazine},
	language = {en},
	month = jan,
	number = {1/2},
	title = {Opening the {{Publication Process}} with {{Executable Research Compendia}}},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1045/january2017-nuest}}

@article{peng2011,
	abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
	author = {Peng, Roger D.},
	chapter = {Perspective},
	copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1126/science.1213847},
	issn = {0036-8075, 1095-9203},
	journal = {Science},
	keywords = {done},
	language = {en},
	month = dec,
	number = {6060},
	pages = {1226--1227},
	pmid = {22144613},
	publisher = {{American Association for the Advancement of Science}},
	title = {Reproducible {{Research}} in {{Computational Science}}},
	volume = {334},
	year = {2011}}

@article{ram2013,
	abstract = {Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.},
	author = {Ram, Karthik},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1186/1751-0473-8-7},
	file = {/Users/ag/Zotero/storage/32WC5YIT/Ram - 2013 - Git can facilitate greater reproducibility and inc.pdf;/Users/ag/Zotero/storage/X3XUNGSV/1751-0473-8-7.html},
	issn = {1751-0473},
	journal = {Source Code for Biology and Medicine},
	month = feb,
	number = {1},
	pages = {7},
	title = {Git Can Facilitate Greater Reproducibility and Increased Transparency in Science},
	volume = {8},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1186/1751-0473-8-7}}

@misc{ramsey,
	author = {Ramsey, Norman},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	file = {/Users/ag/Zotero/storage/VNA44GVT/noweb.html},
	howpublished = {https://www.cs.tufts.edu/\textasciitilde nr/noweb/},
	keywords = {done},
	title = {Noweb Home Page}}

@book{rcoreteam2020,
	address = {{Vienna, Austria}},
	author = {{R Core Team}},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	keywords = {done},
	organization = {{R Foundation for Statistical Computing}},
	title = {R: {{A}} Language and Environment for Statistical Computing},
	year = {2020}}

@book{riederer,
	abstract = {This book showcases short, practical examples of lesser-known tips and tricks to helps users get the most out of these tools. After reading this book, you will understand how R Markdown documents are transformed from plain text and how you may customize nearly every step of this processing. For example, you will learn how to dynamically create content from R code, reference code in other documents or chunks, control the formatting with customer templates, fine-tune how your code is processed, and incorporate multiple languages into your analysis.},
	author = {Riederer, Christophe Dervieux, Emily, Yihui Xie},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	file = {/Users/ag/Zotero/storage/4BIURTCS/rmarkdown-cookbook.html},
	keywords = {done},
	title = {R {{Markdown Cookbook}}}}

@inproceedings{rosenthal1979,
	author = {Rosenthal, R.},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	file = {/Users/ag/Zotero/storage/S6APNZUE/Rosenthal1979PsychBulletin.pdf},
	keywords = {done},
	pages = {638--641},
	publisher = {{Psychological Bulletin}},
	title = {The File Drawer Problem and Tolerance for Null Results.},
	volume = {86},
	year = {1979}}

@book{rstudioteam2020,
	address = {{Boston, MA}},
	author = {{RStudio Team}},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	organization = {{RStudio, PBC.}},
	title = {{{RStudio}}: {{Integrated}} Development Environment for r},
	year = {2020}}

@article{schwab1995,
	abstract = {To organize computational scientific research and hence to conveniently transfer our technology, we impose a simple filing discipline on the authors in our laboratory. A document's makefile includes laboratory-wide standard rules that offer readers these four standard commands: make burn removes the document's result figures, make build recomputes them, make view displays the figures, and make clean removes any intermediate files. Although we developed these standards to aid readers we discovered that authors are often the principal beneficiaries.},
	author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	file = {/Users/ag/Zotero/storage/Q6M53FIZ/Schwab et al. - Reproducible electronic documents.pdf},
	keywords = {done},
	language = {en},
	pages = {14},
	title = {Reproducible Electronic Documents},
	year = {1995}}

@article{simmons2011,
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1177/0956797611417632},
	eprint = {https://doi.org/10.1177/0956797611417632},
	journal = {Psychological Science},
	keywords = {done},
	note = {PMID: 22006061},
	number = {11},
	pages = {1359--1366},
	title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
	volume = {22},
	year = {2011}}

@book{tierney,
	abstract = {A book created for a 3 hour workshop on rmarkdown},
	author = {Tierney, Nicholas},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	file = {/Users/ag/Zotero/storage/XED6DS83/rmd4sci.njtierney.com.html},
	keywords = {done},
	title = {{{RMarkdown}} for {{Scientists}}}}

@article{wikipedia2020,
	abstract = {A meta-analysis is a statistical analysis that combines the results of multiple scientific studies. Meta-analysis can be performed when there are multiple scientific studies addressing the same question, with each individual study reporting measurements that are expected to have some degree of error. The aim then is to use approaches from statistics to derive a pooled estimate closest to the unknown common truth based on how this error is perceived. Existing methods for meta-analysis yield a weighted average from the results of the individual studies, and what differs is the manner in which these weights are allocated and also the manner in which the uncertainty is computed around the point estimate thus generated. In addition to providing an estimate of the unknown common truth, meta-analysis has the capacity to contrast results from different studies and identify patterns among study results, sources of disagreement among those results, or other interesting relationships that may come to light in the context of multiple studies.A key benefit of this approach is the aggregation of information leading to a higher statistical power and more robust point estimate than is possible from the measure derived from any individual study. However, in performing a meta-analysis, an investigator must make choices which can affect the results, including deciding how to search for studies, selecting studies based on a set of objective criteria, dealing with incomplete data, analyzing the data, and accounting for or choosing not to account for publication bias. Judgment calls made in completing a meta-analysis may affect the results. For example, Wanous and colleagues examined four pairs of meta-analyses on the four topics of (a) job performance and satisfaction relationship, (b) realistic job previews, (c) correlates of role conflict and ambiguity, and (d) the job satisfaction and absenteeism relationship, and illustrated how various judgement calls made by the researchers produced different results.Meta-analyses are often, but not always, important components of a systematic review procedure. For instance, a meta-analysis may be conducted on several clinical trials of a medical treatment, in an effort to obtain a better understanding of how well the treatment works. Here it is convenient to follow the terminology used by the Cochrane Collaboration, and use "meta-analysis" to refer to statistical methods of combining evidence, leaving other aspects of 'research synthesis' or 'evidence synthesis', such as combining information from qualitative studies, for the more general context of systematic reviews.  A meta-analysis is a secondary source.},
	annotation = {Page Version ID: 973743271},
	author = {{wikipedia}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	file = {/Users/ag/Zotero/storage/ZZR8GWK6/index.html},
	journal = {Wikipedia},
	keywords = {done},
	language = {en},
	month = aug,
	title = {Meta-Analysis},
	year = {2020}}

@incollection{xie2014,
	author = {Xie, Yihui},
	booktitle = {Implementing Reproducible Computational Research},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
	keywords = {done},
	note = {ISBN 978-1466561595},
	publisher = {{Chapman and Hall/CRC}},
	title = {Knitr: {{A}} Comprehensive Tool for Reproducible Research in {{R}}},
	year = {2014}}

@book{xie2015,
	address = {{Boca Raton, Florida}},
	author = {Xie, Yihui},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	edition = {Second},
	keywords = {done},
	note = {ISBN 978-1498716963},
	publisher = {{Chapman and Hall/CRC}},
	title = {Dynamic Documents with {{R}} and Knitr},
	year = {2015}}

@book{xie2018,
	address = {{Boca Raton, Florida}},
	author = {Xie, Yihui and Allaire, J.J. and Grolemund, Garrett},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	keywords = {done},
	note = {ISBN 9781138359338},
	publisher = {{Chapman and Hall/CRC}},
	title = {R Markdown: {{The}} Definitive Guide},
	year = {2018}}

@manual{xie2020,
	author = {Xie, Yihui},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	keywords = {done},
	note = {R package version 1.29},
	title = {Knitr: {{A}} General-Purpose Package for Dynamic Report Generation in r},
	type = {Manual},
	year = {2020}}

@article{young2008,
	abstract = {The current system of publication in biomedical research provides a distorted view of the reality of scientific data that are generated in the laboratory and clinic. This system can be studied by applying principles from the field of economics. The ``winner's curse,'' a more general statement of publication bias, suggests that the small proportion of results chosen for publication are unrepresentative of scientists' repeated samplings of the real world. The self-correcting mechanism in science is retarded by the extreme imbalance between the abundance of supply (the output of basic science laboratories and clinical investigations) and the increasingly limited venues for publication (journals with sufficiently high impact). This system would be expected intrinsically to lead to the misallocation of resources. The scarcity of available outlets is artificial, based on the costs of printing in an electronic age and a belief that selectivity is equivalent to quality. Science is subject to great uncertainty: we cannot be confident now which efforts will ultimately yield worthwhile achievements. However, the current system abdicates to a small number of intermediates an authoritative prescience to anticipate a highly unpredictable future. In considering society's expectations and our own goals as scientists, we believe that there is a moral imperative to reconsider how scientific data are judged and disseminated.},
	author = {Young, Neal S and Ioannidis, John P. A and {Al-Ubaydli}, Omar},
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	doi = {10.1371/journal.pmed.0050201},
	file = {/Users/ag/Zotero/storage/9NHTTL5J/Young et al. - 2008 - Why Current Publication Practices May Distort Scie.pdf},
	issn = {1549-1676},
	journal = {PLoS Medicine},
	keywords = {done},
	language = {en},
	month = oct,
	number = {10},
	pages = {e201},
	title = {Why {{Current Publication Practices May Distort Science}}},
	volume = {5},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pmed.0050201}}

@misc{AEA,
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:32:25 +0200},
	file = {/Users/ag/Zotero/storage/7K43DVZ3/data.html},
	howpublished = {https://www.aeaweb.org/journals/data},
	title = {American {{Economic Association}}}}

@misc{nsf16137,
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:33:10 +0200},
	file = {/Users/ag/Zotero/storage/8P7URC82/nsf16137.html},
	howpublished = {https://www.nsf.gov/pubs/2016/nsf16137/nsf16137.jsp},
	keywords = {done},
	title = {Dear {{Colleague Letter}}: {{Robust}} and {{Reliable Research}} in the {{Social}}, {{Behavioral}}, and {{Economic Sciences}} (Nsf16137) | {{NSF}} - {{National Science Foundation}}}}

@misc{zotero-436,
	date-added = {2025-09-15 13:30:41 +0200},
	date-modified = {2025-09-15 13:30:41 +0200},
	file = {/Users/ag/Zotero/storage/F83VAQXG/MANUAL.html},
	howpublished = {https://pandoc.org/MANUAL.html},
	title = {Pandoc - {{Pandoc User}}'s {{Guide}}}}

@techreport{broman2018,
	abstract = {Spreadsheets are widely used software tools for data entry, storage, analysis, and visualization. Focusing on the data entry and storage aspects, this paper offers practical recommendations for organizing spreadsheet data to reduce errors and ease later analyses. The basic principles are: be consistent, write dates like YYYY-MM-DD, don't leave any cells empty, put just one thing in a cell, organize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row), create a data dictionary, don't include calculations in the raw data files, don't use font color or highlighting as data, choose good names for things, make backups, use data validation to avoid data entry errors, and save the data in plain text files.},
	author = {Broman, Karl W. and Woo, Kara H.},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	institution = {PeerJ Inc.},
	issn = {2167-9843},
	langid = {english},
	month = sep,
	number = {e3183v2},
	title = {Data Organization in Spreadsheets},
	urldate = {2020-12-04},
	year = {2018}}

@article{celebi2020,
	abstract = {It is essential for the advancement of science that researchers share, reuse and reproduce each other's workflows and protocols. The FAIR principles are a set of guidelines that aim to maximize the value and usefulness of research data, and emphasize the importance of making digital objects findable and reusable by others. The question of how to apply these principles not just to data but also to the workflows and protocols that consume and produce them is still under debate and poses a number of challenges. In this paper we describe a two-fold approach of simultaneously applying the FAIR principles to scientific workflows as well as the involved data. We apply and evaluate our approach on the case of the PREDICT workflow, a highly cited drug repurposing workflow. This includes FAIRification of the involved datasets, as well as applying semantic technologies to represent and store data about the detailed versions of the general protocol, of the concrete workflow instructions, and of their execution traces. We propose a semantic model to address these specific requirements and was evaluated by answering competency questions. This semantic model consists of classes and relations from a number of existing ontologies, including Workflow4ever, PROV, EDAM, and BPMN. This allowed us then to formulate and answer new kinds of competency questions. Our evaluation shows the high degree to which our FAIRified OpenPREDICT workflow now adheres to the FAIR principles and the practicality and usefulness of being able to answer our new competency questions.},
	author = {Celebi, Remzi and Moreira, Joao Rebelo and Hassan, Ahmed A. and Ayyar, Sandeep and Ridder, Lars and Kuhn, Tobias and Dumontier, Michel},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	issn = {2376-5992},
	journal = {PeerJ Computer Science},
	langid = {english},
	month = sep,
	pages = {e281},
	publisher = {PeerJ Inc.},
	shorttitle = {Towards {{FAIR}} Protocols and Workflows},
	title = {Towards {{FAIR}} Protocols and Workflows: The {{OpenPREDICT}} Use Case},
	urldate = {2020-12-04},
	volume = {6},
	year = {2020}}

@techreport{ellis2017,
	abstract = {Within the statistics community, a number of guiding principles for sharing data have emerged; however, these principles are not always made clear to collaborators generating the data. To bridge this divide, we have established a set of guidelines for sharing data. In these, we highlight the need to provide raw data to the statistician, the importance of consistent formatting, and the necessity of including all essential experimental information and pre-processing steps carried out to the statistician. With these guidelines we hope to avoid errors and delays in data analysis.},
	author = {Ellis, Shannon E. and Leek, Jeffrey T.},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	institution = {PeerJ Inc.},
	issn = {2167-9843},
	langid = {english},
	month = sep,
	number = {e3139v5},
	title = {How to Share Data for Collaboration},
	urldate = {2020-12-04},
	year = {2017}}

@article{elmenreich2019,
	abstract = {This article addresses two research questions related to reproducibility within the context of research related to computer science. First, a survey on reproducibility addressed to researchers in the academic and private sectors is described and evaluated. The survey indicates a strong need for open and easily accessible results, in particular, reproducing an experiment should not require too much effort. The results of the survey are then used to formulate guidelines for making research results reproducible. In addition, this article explores four approaches based on software tools that could bring forward reproducibility in research results. After a general analysis of tools, three examples are further investigated based on actual research projects which are used to evaluate previously introduced tools. Results indicate that the evaluated tools contribute well to making simulation results reproducible but due to conflicting requirements, none of the presented solutions fulfills all intended goals perfectly.},
	author = {Elmenreich, Wilfried and Moll, Philipp and Theuermann, Sebastian and Lux, Mathias},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	issn = {2376-5992},
	journal = {PeerJ Computer Science},
	langid = {english},
	month = dec,
	pages = {e240},
	publisher = {PeerJ Inc.},
	title = {Making Simulation Results Reproducible---{{Survey}}, Guidelines, and Examples Based on {{Gradle}} and {{Docker}}},
	urldate = {2020-12-04},
	volume = {5},
	year = {2019}}

@article{gentleman2003,
	abstract = {For various reasons, it is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, etc. with the documents that describe and rely on them. This integration allows readers to both verify and adapt the statements in the documents. Authors can easily reproduce them in the future, and they can present the document's contents in a different medium, e.g. with interactive controls. This paper describes a software framework for authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents, including figures, tables, etc., can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences.},
	author = {Gentleman, Robert and Temple Lang, Duncan},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	issn = {1061-8600, 1537-2715},
	journal = {Journal of Computational and Graphical Statistics},
	langid = {english},
	number = {1},
	pages = {1--23},
	title = {Statistical {{Analyses}} and {{Reproducible Research}}},
	urldate = {2020-08-23},
	volume = {16},
	year = {2003}}

@article{gentleman2004,
	author = {Gentleman, Robert and Lang, Duncan Temple},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	journal = {Bioconductor Project Working Papers},
	month = may,
	title = {Statistical {{Analyses}} and {{Reproducible Research}}},
	year = {2004}}

@article{opensciencecollaboration2015,
	abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt; .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that we already know this belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	author = {Open Science Collaboration},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	elocation-id = {aac4716},
	eprint = {https://science.sciencemag.org/content/349/6251/aac4716.full.pdf},
	issn = {0036-8075},
	journal = {Science},
	number = {6251},
	publisher = {American Association for the Advancement of Science},
	title = {Estimating the Reproducibility of Psychological Science},
	volume = {349},
	year = {2015}}

@article{piwowar2018,
	abstract = {Despite growing interest in Open Access (OA) to scholarly literature, there is an unmet need for large-scale, up-to-date, and reproducible studies assessing the prevalence and characteristics of OA. We address this need using oaDOI, an open online service that determines OA status for 67 million articles. We use three samples, each of 100,000 articles, to investigate OA in three populations: (1) all journal articles assigned a Crossref DOI, (2) recent journal articles indexed in Web of Science, and (3) articles viewed by users of Unpaywall, an open-source browser extension that lets users find OA articles using oaDOI. We estimate that at least 28\% of the scholarly literature is OA (19M in total) and that this proportion is growing, driven particularly by growth in Gold and Hybrid. The most recent year analyzed (2015) also has the highest percentage of OA (45\%). Because of this growth, and the fact that readers disproportionately access newer articles, we find that Unpaywall users encounter OA quite frequently: 47\% of articles they view are OA. Notably, the most common mechanism for OA is not Gold, Green, or Hybrid OA, but rather an under-discussed category we dub Bronze: articles made free-to-read on the publisher website, without an explicit Open license. We also examine the citation impact of OA articles, corroborating the so-called open-access citation advantage: accounting for age and discipline, OA articles receive 18\% more citations than average, an effect driven primarily by Green and Hybrid OA. We encourage further research using the free oaDOI service, as a way to inform OA policy and practice.},
	author = {Piwowar, Heather and Priem, Jason and Larivi{\`e}re, Vincent and Alperin, Juan Pablo and Matthias, Lisa and Norlander, Bree and Farley, Ashley and West, Jevin and Haustein, Stefanie},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	issn = {2167-8359},
	journal = {PeerJ},
	langid = {english},
	month = feb,
	pages = {e4375},
	publisher = {PeerJ Inc.},
	shorttitle = {The State of {{OA}}},
	title = {The State of {{OA}}: A Large-Scale Analysis of the Prevalence and Impact of {{Open Access}} Articles},
	urldate = {2020-12-04},
	volume = {6},
	year = {2018}}

@article{starr2015,
	abstract = {Reproducibility and reusability of research results is an important concern in scientific communication and science policy. A foundational element of reproducibility and reusability is the open and persistently available presentation of research data. However, many common approaches for primary data publication in use today do not achieve sufficient long-term robustness, openness, accessibility or uniformity. Nor do they permit comprehensive exploitation by modern Web technologies. This has led to several authoritative studies recommending uniform direct citation of data archived in persistent repositories. Data are to be considered as first-class scholarly objects, and treated similarly in many ways to cited and archived scientific and scholarly literature. Here we briefly review the most current and widely agreed set of principle-based recommendations for scholarly data citation, the Joint Declaration of Data Citation Principles (JDDCP). We then present a framework for operationalizing the JDDCP; and a set of initial recommendations on identifier schemes, identifier resolution behavior, required metadata elements, and best practices for realizing programmatic machine actionability of cited data. The main target audience for the common implementation guidelines in this article consists of publishers, scholarly organizations, and persistent data repositories, including technical staff members in these organizations. But ordinary researchers can also benefit from these recommendations. The guidance provided here is intended to help achieve widespread, uniform human and machine accessibility of deposited data, in support of significantly improved verification, validation, reproducibility and re-use of scholarly/scientific data.},
	author = {Starr, Joan and Castro, Eleni and Crosas, Merc{\`e} and Dumontier, Michel and Downs, Robert R. and Duerr, Ruth and Haak, Laurel L. and Haendel, Melissa and Herman, Ivan and Hodson, Simon and Hourcl{\'e}, Joe and Kratz, John Ernest and Lin, Jennifer and Nielsen, Lars Holm and Nurnberger, Amy and Proell, Stefan and Rauber, Andreas and Sacchi, Simone and Smith, Arthur and Taylor, Mike and Clark, Tim},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	issn = {2376-5992},
	journal = {PeerJ Computer Science},
	langid = {english},
	month = may,
	pages = {e1},
	publisher = {PeerJ Inc.},
	title = {Achieving Human and Machine Accessibility of Cited Data in Scholarly Publications},
	urldate = {2020-12-04},
	volume = {1},
	year = {2015}}

@article{steiniger2013,
	abstract = {Over the last decade an increasing number of free and open source software projects have been founded that concentrate on developing several types of software for geographic data collection, storage, analysis and visualization. We first identify the drivers of such software projects and identify different types of geographic information software, e.g. desktop GIS, remote sensing software, server GIS etc. We then list the major projects for each software category. Afterwards we discuss the points that should be considered if free and open source software is to be selected for use in business and research, such as software functionality, license types and their restrictions, developer and user community characteristics, etc. Finally possible future developments are addressed.},
	author = {Steiniger, Stefan and Hunter, Andrew J. S.},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	issn = {0198-9715},
	journal = {Computers, Environment and Urban Systems},
	keywords = {FOSS4G,Free software,GIS software,Open source,Overview,Software selection},
	langid = {english},
	month = may,
	pages = {136--150},
	title = {The 2012 Free and Open Source {{GIS}} Software Map -- {{A}} Guide to Facilitate Research, Development, and Adoption},
	urldate = {2020-12-04},
	volume = {39},
	year = {2013}}

@book{wickham2016,
	address = {Beijing},
	author = {Wickham, Hadley and Grolemund, Garrett},
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	isbn = {978-1-4919-1039-9},
	keywords = {Computer programs; Information visualization,Computer programs; R (Computer program language); Statistiske metoder; R-sprak; programmering; R; statistikk; data science; datavitenskap,Data mining},
	langid = {english},
	lccn = {006.312 Wic, 519.2 W63r, 006.312 WIC},
	pages = {XXV, 492},
	publisher = {O'Reilly},
	shorttitle = {R for Data Science},
	title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
	year = {2016}}

@misc{zotero-207,
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	howpublished = {https://pandoc.org/MANUAL.html},
	title = {Pandoc - {{Pandoc User}}'s {{Guide}}},
	urldate = {2020-03-10}}

@misc{sosi1.0,
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:37:31 +0200},
	howpublished = {https://www.nb.no/nbsok/nb/62d23fb9bb9c18f9c57f88f8945e30ef.nbdigital?lang=no\#5},
	title = {Forslag Til Utvekslingsformat for Digitale Geodata ({{SOSI-formatet}}, Versjon 1.0) - {{Nasjonalbiblioteket}}},
	urldate = {2020-12-14}}

@misc{zotero-381,
	date-added = {2025-09-15 13:30:08 +0200},
	date-modified = {2025-09-15 13:30:08 +0200},
	howpublished = {https://medium.com/@miles.mcbain/tidying-the-australian-same-sex-marriage-postal-survey-data-with-r-5d35cea07962},
	title = {Tidying the {{Australian Same Sex Marriage Postal Survey Data}} with {{R}} {\textbar} by {{Miles McBain}} {\textbar} {{Medium}}},
	urldate = {2020-12-04}}

@article{begley2012raise,
	author = {Begley, C Glenn and Ellis, Lee M},
	journal = {Nature},
	number = {7391},
	pages = {531--533},
	publisher = {Nature Publishing Group UK London},
	title = {Raise standards for preclinical cancer research},
	volume = {483},
	year = {2012}}

@article{hardwicke2020calibrating,
	author = {Hardwicke, Tom E and Serghiou, Stylianos and Janiaud, Perrine and Danchev, Valentin and Cr{\"u}well, Sophia and Goodman, Steven N and Ioannidis, John PA},
	journal = {Annual Review of Statistics and Its Application},
	number = {1},
	pages = {11--37},
	publisher = {Annual Reviews},
	title = {Calibrating the scientific ecosystem through meta-research},
	volume = {7},
	year = {2020}}

@article{baker2016,
	author = {Baker, Monya},
	doi = {10.1038/533452a},
	journal = {Nature},
	pages = {452--454},
	title = {1,500 Scientists Lift the Lid on Reproducibility},
	volume = {533},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1038/533452a}}

@inproceedings{rule2018exploration,
	author = {Rule, Adam and Tabard, Aur{\'e}lien and Hollan, James D},
	booktitle = {Proceedings of the 2018 CHI conference on human factors in computing systems},
	pages = {1--12},
	title = {Exploration and explanation in computational notebooks},
	year = {2018}}

@article{munafo2017manifesto,
	author = {Munaf{\`o}, Marcus R and Nosek, Brian A and Bishop, Dorothy VM and Button, Katherine S and Chambers, Christopher D and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J and Ioannidis, John PA},
	journal = {Nature human behaviour},
	number = {1},
	pages = {0021},
	publisher = {Nature Publishing Group UK London},
	title = {A manifesto for reproducible science},
	volume = {1},
	year = {2017}}

@inproceedings{pimentel2019large,
	author = {Pimentel, Jo{\~a}o Felipe and Murta, Leonardo and Braganholo, Vanessa and Freire, Juliana},
	booktitle = {2019 IEEE/ACM 16th international conference on mining software repositories (MSR)},
	organization = {IEEE},
	pages = {507--517},
	title = {A large-scale study about quality and reproducibility of jupyter notebooks},
	year = {2019}}
